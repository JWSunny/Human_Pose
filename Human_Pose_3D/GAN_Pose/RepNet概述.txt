本节主要介绍一篇2D转3D姿态估计的论文：
RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation （2019CVPR）

主要的模型结构可视化参考 RepNet_model.jpg
使用keras进行实现，主要参考 RepNet_model.py文件；

下面主要是对模型中模块的解释：

    1.第1阶段主要是两个独立的分支，输入的都是 2D 姿态坐标；

    2.分支 1 是姿态生成网络，主要由 首尾全连接模块 和 中间的3个全连接残差模块构成，每个全连接都是
    选择的1000个节点，全连接之后使用的都是 LeakyRelu激活函数；最终网络的输出是 3D估计坐标；

    3.分支 2 同样是使用的 2个残差模块 和 1个全连接快组成，最终学习的相机参数，称作相机参数网络；

    4.上述俩个分支学习的结果进行组合，同时经过 重投影网络，投影到2D姿态，与输入的 2D姿态计算最终的
    损失；这边损失使用的是L2损失；

    5.预测的3d，需要通过判别模型进行判别，与真实的3d坐标进行比较；该判别网络模型，主要由两个分支
    组成，一个分支是运动链空间层+全连接层，一个分支是全连接层，两者特征进行拼接，接全连接层最后
    与 真实的3D坐标计算损失函数；

    6.总共包含的损失，主要包括 3块损失，一块是 相机参数损失， 一块是重投影到2D空间的 2D姿态估计损失；
    另一块是 3D判别模型的损失，计算生成的3D姿态 与 真实的3D姿态坐标之间的损失；


